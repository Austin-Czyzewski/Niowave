{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from openpyxl import Workbook\n",
    "from openpyxl import load_workbook\n",
    "import matplotlib as mpl\n",
    "import re\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "Time = time.time()\n",
    "mpl.rcParams['savefig.dpi']  = 500\n",
    "mpl.rcParams['font.size']    = 12\n",
    "mpl.rcParams['mathtext.fontset'] = 'stix'\n",
    "mpl.rcParams['font.family'] = 'STIXGeneral'\n",
    "\n",
    "# filename = \"UTA-2_NU_Inventory_1cycle_24hr.out\"\n",
    "filename = \"UTA-2_LEU_Inventory_5yr_12hr.out\"\n",
    "\n",
    "with open(filename, 'r') as file:\n",
    "    first_file_string = file.read()#.replace('\\n', '')\n",
    "    file.close()\n",
    "        \n",
    "first_file_string = re.sub(r'(\\d{1}.\\d{4})(-\\d{3})', r'\\1E\\2', first_file_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132 Cases\n"
     ]
    }
   ],
   "source": [
    "# Thing = re.findall(r't=\\[\\d{,3}\\s{1}\\d{,2}\\s{1}\\d{,3}\\]', first_file_string)\n",
    "Case_Intervals = re.findall(r't\\s*=\\s*\\[[^\\]]*\\]', first_file_string)\n",
    "\n",
    "Integer_Interval_List = list()\n",
    "\n",
    "for Interval in Case_Intervals:\n",
    "    if 'i' in Interval:\n",
    "        Integer_Interval_List.append(int(re.split('\\[|i',Interval)[1]))\n",
    "        \n",
    "print(\"{} Cases\".format(len(Integer_Interval_List)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating DataFrames from\n",
      "['  0.0E+00', '  6.4E-01', '  6.4E-01', '  1.3E+00', '  1.9E+00', '  2.5E+00', '  3.2E+00', '  3.8E+00', '  4.5E+00', '  5.1E+00', '  5.7E+00', '  6.4E+00', '  7.0E+00', '   ']\n",
      "['  7.000', '  8.615', '  8.615', '  10.231', '  11.846', '  13.462', '  15.077', '  16.692', '  18.308', '  19.923', '  21.538', '  23.154', '  24.769', '  26.385', '  28.000', '   ']\n",
      "['  28.000', '  28.636', '  28.636', '  29.273', '  29.909', '  30.545', '  31.182', '  31.818', '  32.455', '  33.091', '  33.727', '  34.364', '  35.000', '   ']\n",
      "['  35.000', '  36.615', '  36.615', '  38.231', '  39.846', '  41.462', '  43.077', '  44.692', '  46.308', '  47.923', '  49.538', '  51.154', '  52.769', '  54.385', '  56.000', '   ']\n",
      "['  56.000', '  56.636', '  56.636', '  57.273', '  57.909', '  58.545', '  59.182', '  59.818', '  60.455', '  61.091', '  61.727', '  62.364', '  63.000', '   ']\n",
      "['  63.000', '  64.615', '  64.615', '  66.231', '  67.846', '  69.462', '  71.077', '  72.692', '  74.308', '  75.923', '  77.538', '  79.154', '  80.769', '  82.385', '  84.000', '   ']\n",
      "['  84.000', '  84.636', '  84.636', '  85.273', '  85.909', '  86.545', '  87.182', '  87.818', '  88.455', '  89.091', '  89.727', '  90.364', '  91.000', '   ']\n",
      "['  91.000', '  92.615', '  92.615', '  94.231', '  95.846', '  97.462', '  99.077', '  100.692', '  102.308', '  103.923', '  105.538', '  107.154', '  108.769', '  110.385', '  112.000', '   ']\n",
      "['  112.000', '  112.636', '  112.636', '  113.273', '  113.909', '  114.545', '  115.182', '  115.818', '  116.455', '  117.091', '  117.727', '  118.364', '  119.000', '   ']\n",
      "['  119.000', '  120.615', '  120.615', '  122.231', '  123.846', '  125.462', '  127.077', '  128.692', '  130.308', '  131.923', '  133.538', '  135.154', '  136.769', '  138.385', '  140.000', '   ']\n",
      "['  140.000', '  140.636', '  140.636', '  141.273', '  141.909', '  142.545', '  143.182', '  143.818', '  144.455', '  145.091', '  145.727', '  146.364', '  147.000', '   ']\n",
      "['  147.000', '  148.615', '  148.615', '  150.231', '  151.846', '  153.462', '  155.077', '  156.692', '  158.308', '  159.923', '  161.538', '  163.154', '  164.769', '  166.385', '  168.000', '   ']\n",
      "['  168.000', '  168.636', '  168.636', '  169.273', '  169.909', '  170.545', '  171.182', '  171.818', '  172.455', '  173.091', '  173.727', '  174.364', '  175.000', '   ']\n",
      "['  175.000', '  176.615', '  176.615', '  178.231', '  179.846', '  181.462', '  183.077', '  184.692', '  186.308', '  187.923', '  189.538', '  191.154', '  192.769', '  194.385', '  196.000', '   ']\n",
      "['  196.000', '  196.636', '  196.636', '  197.273', '  197.909', '  198.545', '  199.182', '  199.818', '  200.455', '  201.091', '  201.727', '  202.364', '  203.000', '   ']\n",
      "['  203.000', '  204.615', '  204.615', '  206.231', '  207.846', '  209.462', '  211.077', '  212.692', '  214.308', '  215.923', '  217.538', '  219.154', '  220.769', '  222.385', '  224.000', '   ']\n",
      "['  224.000', '  224.636', '  224.636', '  225.273', '  225.909', '  226.545', '  227.182', '  227.818', '  228.455', '  229.091', '  229.727', '  230.364', '  231.000', '   ']\n",
      "['  231.000', '  232.615', '  232.615', '  234.231', '  235.846', '  237.462', '  239.077', '  240.692', '  242.308', '  243.923', '  245.538', '  247.154', '  248.769', '  250.385', '  252.000', '   ']\n",
      "['  252.000', '  252.636', '  252.636', '  253.273', '  253.909', '  254.545', '  255.182', '  255.818', '  256.455', '  257.091', '  257.727', '  258.364', '  259.000', '   ']\n",
      "['  259.000', '  260.615', '  260.615', '  262.231', '  263.846', '  265.462', '  267.077', '  268.692', '  270.308', '  271.923', '  273.538', '  275.154', '  276.769', '  278.385', '  280.000', '   ']\n",
      "['  280.000', '  280.636', '  280.636', '  281.273', '  281.909', '  282.545', '  283.182', '  283.818', '  284.455', '  285.091', '  285.727', '  286.364', '  287.000', '   ']\n",
      "['  287.000', '  288.615', '  288.615', '  290.231', '  291.846', '  293.462', '  295.077', '  296.692', '  298.308', '  299.923', '  301.538', '  303.154', '  304.769', '  306.385', '  308.000', '   ']\n",
      "['  308.000', '  308.636', '  308.636', '  309.273', '  309.909', '  310.545', '  311.182', '  311.818', '  312.455', '  313.091', '  313.727', '  314.364', '  315.000', '   ']\n",
      "['  315.000', '  316.615', '  316.615', '  318.231', '  319.846', '  321.462', '  323.077', '  324.692', '  326.308', '  327.923', '  329.538', '  331.154', '  332.769', '  334.385', '  336.000', '   ']\n",
      "['  336.000', '  336.636', '  336.636', '  337.273', '  337.909', '  338.545', '  339.182', '  339.818', '  340.455', '  341.091', '  341.727', '  342.364', '  343.000', '   ']\n",
      "['  343.000', '  344.615', '  344.615', '  346.231', '  347.846', '  349.462', '  351.077', '  352.692', '  354.308', '  355.923', '  357.538', '  359.154', '  360.769', '  362.385', '  364.000', '   ']\n",
      "['  364.000', '  364.636', '  364.636', '  365.273', '  365.909', '  366.545', '  367.182', '  367.818', '  368.455', '  369.091', '  369.727', '  370.364', '  371.000', '   ']\n",
      "['  371.000', '  372.615', '  372.615', '  374.231', '  375.846', '  377.462', '  379.077', '  380.692', '  382.308', '  383.923', '  385.538', '  387.154', '  388.769', '  390.385', '  392.000', '   ']\n",
      "['  392.000', '  392.636', '  392.636', '  393.273', '  393.909', '  394.545', '  395.182', '  395.818', '  396.455', '  397.091', '  397.727', '  398.364', '  399.000', '   ']\n",
      "['  399.000', '  400.615', '  400.615', '  402.231', '  403.846', '  405.462', '  407.077', '  408.692', '  410.308', '  411.923', '  413.538', '  415.154', '  416.769', '  418.385', '  420.000', '   ']\n",
      "['  420.000', '  420.636', '  420.636', '  421.273', '  421.909', '  422.545', '  423.182', '  423.818', '  424.455', '  425.091', '  425.727', '  426.364', '  427.000', '   ']\n",
      "['  427.000', '  428.615', '  428.615', '  430.231', '  431.846', '  433.462', '  435.077', '  436.692', '  438.308', '  439.923', '  441.538', '  443.154', '  444.769', '  446.385', '  448.000', '   ']\n",
      "['  448.000', '  448.636', '  448.636', '  449.273', '  449.909', '  450.545', '  451.182', '  451.818', '  452.455', '  453.091', '  453.727', '  454.364', '  455.000', '   ']\n",
      "['  455.000', '  456.615', '  456.615', '  458.231', '  459.846', '  461.462', '  463.077', '  464.692', '  466.308', '  467.923', '  469.538', '  471.154', '  472.769', '  474.385', '  476.000', '   ']\n",
      "['  476.000', '  476.636', '  476.636', '  477.273', '  477.909', '  478.545', '  479.182', '  479.818', '  480.455', '  481.091', '  481.727', '  482.364', '  483.000', '   ']\n",
      "['  483.000', '  484.615', '  484.615', '  486.231', '  487.846', '  489.462', '  491.077', '  492.692', '  494.308', '  495.923', '  497.538', '  499.154', '  500.769', '  502.385', '  504.000', '   ']\n",
      "['  504.000', '  504.636', '  504.636', '  505.273', '  505.909', '  506.545', '  507.182', '  507.818', '  508.455', '  509.091', '  509.727', '  510.364', '  511.000', '   ']\n",
      "['  511.000', '  512.615', '  512.615', '  514.231', '  515.846', '  517.462', '  519.077', '  520.692', '  522.308', '  523.923', '  525.538', '  527.154', '  528.769', '  530.385', '  532.000', '   ']\n",
      "['  532.000', '  532.636', '  532.636', '  533.273', '  533.909', '  534.545', '  535.182', '  535.818', '  536.455', '  537.091', '  537.727', '  538.364', '  539.000', '   ']\n",
      "['  539.000', '  540.615', '  540.615', '  542.231', '  543.846', '  545.462', '  547.077', '  548.692', '  550.308', '  551.923', '  553.538', '  555.154', '  556.769', '  558.385', '  560.000', '   ']\n",
      "['  560.000', '  560.636', '  560.636', '  561.273', '  561.909', '  562.545', '  563.182', '  563.818', '  564.455', '  565.091', '  565.727', '  566.364', '  567.000', '   ']\n",
      "['  567.000', '  568.615', '  568.615', '  570.231', '  571.846', '  573.462', '  575.077', '  576.692', '  578.308', '  579.923', '  581.538', '  583.154', '  584.769', '  586.385', '  588.000', '   ']\n",
      "['  588.000', '  588.636', '  588.636', '  589.273', '  589.909', '  590.545', '  591.182', '  591.818', '  592.455', '  593.091', '  593.727', '  594.364', '  595.000', '   ']\n",
      "['  595.000', '  596.615', '  596.615', '  598.231', '  599.846', '  601.462', '  603.077', '  604.692', '  606.308', '  607.923', '  609.538', '  611.154', '  612.769', '  614.385', '  616.000', '   ']\n",
      "['  616.000', '  616.636', '  616.636', '  617.273', '  617.909', '  618.545', '  619.182', '  619.818', '  620.455', '  621.091', '  621.727', '  622.364', '  623.000', '   ']\n",
      "['  623.000', '  624.615', '  624.615', '  626.231', '  627.846', '  629.462', '  631.077', '  632.692', '  634.308', '  635.923', '  637.538', '  639.154', '  640.769', '  642.385', '  644.000', '   ']\n",
      "['  644.000', '  644.636', '  644.636', '  645.273', '  645.909', '  646.545', '  647.182', '  647.818', '  648.455', '  649.091', '  649.727', '  650.364', '  651.000', '   ']\n",
      "['  651.000', '  652.615', '  652.615', '  654.231', '  655.846', '  657.462', '  659.077', '  660.692', '  662.308', '  663.923', '  665.538', '  667.154', '  668.769', '  670.385', '  672.000', '   ']\n",
      "['  672.000', '  672.636', '  672.636', '  673.273', '  673.909', '  674.545', '  675.182', '  675.818', '  676.455', '  677.091', '  677.727', '  678.364', '  679.000', '   ']\n",
      "['  679.000', '  680.615', '  680.615', '  682.231', '  683.846', '  685.462', '  687.077', '  688.692', '  690.308', '  691.923', '  693.538', '  695.154', '  696.769', '  698.385', '  700.000', '   ']\n",
      "['  700.000', '  700.636', '  700.636', '  701.273', '  701.909', '  702.545', '  703.182', '  703.818', '  704.455', '  705.091', '  705.727', '  706.364', '  707.000', '   ']\n",
      "['  707.000', '  708.615', '  708.615', '  710.231', '  711.846', '  713.462', '  715.077', '  716.692', '  718.308', '  719.923', '  721.538', '  723.154', '  724.769', '  726.385', '  728.000', '   ']\n",
      "['  728.000', '  728.636', '  728.636', '  729.273', '  729.909', '  730.545', '  731.182', '  731.818', '  732.455', '  733.091', '  733.727', '  734.364', '  735.000', '   ']\n",
      "['  735.000', '  736.615', '  736.615', '  738.231', '  739.846', '  741.462', '  743.077', '  744.692', '  746.308', '  747.923', '  749.538', '  751.154', '  752.769', '  754.385', '  756.000', '   ']\n",
      "['  756.000', '  756.636', '  756.636', '  757.273', '  757.909', '  758.545', '  759.182', '  759.818', '  760.455', '  761.091', '  761.727', '  762.364', '  763.000', '   ']\n",
      "['  763.000', '  764.615', '  764.615', '  766.231', '  767.846', '  769.462', '  771.077', '  772.692', '  774.308', '  775.923', '  777.538', '  779.154', '  780.769', '  782.385', '  784.000', '   ']\n",
      "['  784.000', '  784.636', '  784.636', '  785.273', '  785.909', '  786.545', '  787.182', '  787.818', '  788.455', '  789.091', '  789.727', '  790.364', '  791.000', '   ']\n",
      "['  791.000', '  792.615', '  792.615', '  794.231', '  795.846', '  797.462', '  799.077', '  800.692', '  802.308', '  803.923', '  805.538', '  807.154', '  808.769', '  810.385', '  812.000', '   ']\n",
      "['  812.000', '  812.636', '  812.636', '  813.273', '  813.909', '  814.545', '  815.182', '  815.818', '  816.455', '  817.091', '  817.727', '  818.364', '  819.000', '   ']\n",
      "['  819.000', '  820.615', '  820.615', '  822.231', '  823.846', '  825.462', '  827.077', '  828.692', '  830.308', '  831.923', '  833.538', '  835.154', '  836.769', '  838.385', '  840.000', '   ']\n",
      "['  840.000', '  840.636', '  840.636', '  841.273', '  841.909', '  842.545', '  843.182', '  843.818', '  844.455', '  845.091', '  845.727', '  846.364', '  847.000', '   ']\n",
      "['  847.000', '  848.615', '  848.615', '  850.231', '  851.846', '  853.462', '  855.077', '  856.692', '  858.308', '  859.923', '  861.538', '  863.154', '  864.769', '  866.385', '  868.000', '   ']\n",
      "['  868.000', '  868.636', '  868.636', '  869.273', '  869.909', '  870.545', '  871.182', '  871.818', '  872.455', '  873.091', '  873.727', '  874.364', '  875.000', '   ']\n",
      "['  875.000', '  876.615', '  876.615', '  878.231', '  879.846', '  881.462', '  883.077', '  884.692', '  886.308', '  887.923', '  889.538', '  891.154', '  892.769', '  894.385', '  896.000', '   ']\n",
      "['  896.000', '  896.636', '  896.636', '  897.273', '  897.909', '  898.545', '  899.182', '  899.818', '  900.455', '  901.091', '  901.727', '  902.364', '  903.000', '   ']\n",
      "['  903.000', '  904.615', '  904.615', '  906.231', '  907.846', '  909.462', '  911.077', '  912.692', '  914.308', '  915.923', '  917.538', '  919.154', '  920.769', '  922.385', '  924.000', '   ']\n",
      "['  924.000', '  924.636', '  924.636', '  925.273', '  925.909', '  926.545', '  927.182', '  927.818', '  928.455', '  929.091', '  929.727', '  930.364', '  931.000', '   ']\n",
      "['  931.000', '  932.615', '  932.615', '  934.231', '  935.846', '  937.462', '  939.077', '  940.692', '  942.308', '  943.923', '  945.538', '  947.154', '  948.769', '  950.385', '  952.000', '   ']\n",
      "['  952.000', '  952.636', '  952.636', '  953.273', '  953.909', '  954.545', '  955.182', '  955.818', '  956.455', '  957.091', '  957.727', '  958.364', '  959.000', '   ']\n",
      "['  959.000', '  960.615', '  960.615', '  962.231', '  963.846', '  965.462', '  967.077', '  968.692', '  970.308', '  971.923', '  973.538', '  975.154', '  976.769', '  978.385', '  980.000', '   ']\n",
      "['  980.000', '  980.636', '  980.636', '  981.273', '  981.909', '  982.545', '  983.182', '  983.818', '  984.455', '  985.091', '  985.727', '  986.364', '  987.000', '   ']\n",
      "['  987.000', '  988.615', '  988.615', '  990.231', '  991.846', '  993.462', '  995.077', '  996.692', '  998.308', '  999.923', '  1001.538', '  1003.154', '  1004.769', '  1006.385', '  1008.000', '   ']\n",
      "['  1008.000', '  1008.636', '  1008.636', '  1009.273', '  1009.909', '  1010.545', '  1011.182', '  1011.818', '  1012.455', '  1013.091', '  1013.727', '  1014.364', '  1015.000', '   ']\n",
      "['  1015.000', '  1016.615', '  1016.615', '  1018.231', '  1019.846', '  1021.462', '  1023.077', '  1024.692', '  1026.308', '  1027.923', '  1029.538', '  1031.154', '  1032.769', '  1034.385', '  1036.000', '   ']\n",
      "['  1036.000', '  1036.636', '  1036.636', '  1037.273', '  1037.909', '  1038.545', '  1039.182', '  1039.818', '  1040.455', '  1041.091', '  1041.727', '  1042.364', '  1043.000', '   ']\n",
      "['  1043.000', '  1044.615', '  1044.615', '  1046.231', '  1047.846', '  1049.462', '  1051.077', '  1052.692', '  1054.308', '  1055.923', '  1057.538', '  1059.154', '  1060.769', '  1062.385', '  1064.000', '   ']\n",
      "['  1064.000', '  1064.636', '  1064.636', '  1065.273', '  1065.909', '  1066.545', '  1067.182', '  1067.818', '  1068.455', '  1069.091', '  1069.727', '  1070.364', '  1071.000', '   ']\n",
      "['  1071.000', '  1072.615', '  1072.615', '  1074.231', '  1075.846', '  1077.462', '  1079.077', '  1080.692', '  1082.308', '  1083.923', '  1085.538', '  1087.154', '  1088.769', '  1090.385', '  1092.000', '   ']\n",
      "['  1092.000', '  1092.636', '  1092.636', '  1093.273', '  1093.909', '  1094.545', '  1095.182', '  1095.818', '  1096.455', '  1097.091', '  1097.727', '  1098.364', '  1099.000', '   ']\n",
      "['  1099.000', '  1100.615', '  1100.615', '  1102.231', '  1103.846', '  1105.462', '  1107.077', '  1108.692', '  1110.308', '  1111.923', '  1113.538', '  1115.154', '  1116.769', '  1118.385', '  1120.000', '   ']\n",
      "['  1120.000', '  1120.636', '  1120.636', '  1121.273', '  1121.909', '  1122.545', '  1123.182', '  1123.818', '  1124.455', '  1125.091', '  1125.727', '  1126.364', '  1127.000', '   ']\n",
      "['  1127.000', '  1128.615', '  1128.615', '  1130.231', '  1131.846', '  1133.462', '  1135.077', '  1136.692', '  1138.308', '  1139.923', '  1141.538', '  1143.154', '  1144.769', '  1146.385', '  1148.000', '   ']\n",
      "['  1148.000', '  1148.636', '  1148.636', '  1149.273', '  1149.909', '  1150.545', '  1151.182', '  1151.818', '  1152.455', '  1153.091', '  1153.727', '  1154.364', '  1155.000', '   ']\n",
      "['  1155.000', '  1156.615', '  1156.615', '  1158.231', '  1159.846', '  1161.462', '  1163.077', '  1164.692', '  1166.308', '  1167.923', '  1169.538', '  1171.154', '  1172.769', '  1174.385', '  1176.000', '   ']\n",
      "['  1176.000', '  1176.636', '  1176.636', '  1177.273', '  1177.909', '  1178.545', '  1179.182', '  1179.818', '  1180.455', '  1181.091', '  1181.727', '  1182.364', '  1183.000', '   ']\n",
      "['  1183.000', '  1184.615', '  1184.615', '  1186.231', '  1187.846', '  1189.462', '  1191.077', '  1192.692', '  1194.308', '  1195.923', '  1197.538', '  1199.154', '  1200.769', '  1202.385', '  1204.000', '   ']\n",
      "['  1204.000', '  1204.636', '  1204.636', '  1205.273', '  1205.909', '  1206.545', '  1207.182', '  1207.818', '  1208.455', '  1209.091', '  1209.727', '  1210.364', '  1211.000', '   ']\n",
      "['  1211.000', '  1212.615', '  1212.615', '  1214.231', '  1215.846', '  1217.462', '  1219.077', '  1220.692', '  1222.308', '  1223.923', '  1225.538', '  1227.154', '  1228.769', '  1230.385', '  1232.000', '   ']\n",
      "['  1232.000', '  1232.636', '  1232.636', '  1233.273', '  1233.909', '  1234.545', '  1235.182', '  1235.818', '  1236.455', '  1237.091', '  1237.727', '  1238.364', '  1239.000', '   ']\n",
      "['  1239.000', '  1240.615', '  1240.615', '  1242.231', '  1243.846', '  1245.462', '  1247.077', '  1248.692', '  1250.308', '  1251.923', '  1253.538', '  1255.154', '  1256.769', '  1258.385', '  1260.000', '   ']\n",
      "['  1260.000', '  1260.636', '  1260.636', '  1261.273', '  1261.909', '  1262.545', '  1263.182', '  1263.818', '  1264.455', '  1265.091', '  1265.727', '  1266.364', '  1267.000', '   ']\n",
      "['  1267.000', '  1268.615', '  1268.615', '  1270.231', '  1271.846', '  1273.462', '  1275.077', '  1276.692', '  1278.308', '  1279.923', '  1281.538', '  1283.154', '  1284.769', '  1286.385', '  1288.000', '   ']\n",
      "['  1288.000', '  1288.636', '  1288.636', '  1289.273', '  1289.909', '  1290.545', '  1291.182', '  1291.818', '  1292.455', '  1293.091', '  1293.727', '  1294.364', '  1295.000', '   ']\n",
      "['  1295.000', '  1296.615', '  1296.615', '  1298.231', '  1299.846', '  1301.462', '  1303.077', '  1304.692', '  1306.308', '  1307.923', '  1309.538', '  1311.154', '  1312.769', '  1314.385', '  1316.000', '   ']\n",
      "['  1316.000', '  1316.636', '  1316.636', '  1317.273', '  1317.909', '  1318.545', '  1319.182', '  1319.818', '  1320.455', '  1321.091', '  1321.727', '  1322.364', '  1323.000', '   ']\n",
      "['  1323.000', '  1324.615', '  1324.615', '  1326.231', '  1327.846', '  1329.462', '  1331.077', '  1332.692', '  1334.308', '  1335.923', '  1337.538', '  1339.154', '  1340.769', '  1342.385', '  1344.000', '   ']\n",
      "['  1344.000', '  1344.636', '  1344.636', '  1345.273', '  1345.909', '  1346.545', '  1347.182', '  1347.818', '  1348.455', '  1349.091', '  1349.727', '  1350.364', '  1351.000', '   ']\n",
      "['  1351.000', '  1352.615', '  1352.615', '  1354.231', '  1355.846', '  1357.462', '  1359.077', '  1360.692', '  1362.308', '  1363.923', '  1365.538', '  1367.154', '  1368.769', '  1370.385', '  1372.000', '   ']\n",
      "['  1372.000', '  1372.636', '  1372.636', '  1373.273', '  1373.909', '  1374.545', '  1375.182', '  1375.818', '  1376.455', '  1377.091', '  1377.727', '  1378.364', '  1379.000', '   ']\n",
      "['  1379.000', '  1380.615', '  1380.615', '  1382.231', '  1383.846', '  1385.462', '  1387.077', '  1388.692', '  1390.308', '  1391.923', '  1393.538', '  1395.154', '  1396.769', '  1398.385', '  1400.000', '   ']\n",
      "['  1400.000', '  1400.636', '  1400.636', '  1401.273', '  1401.909', '  1402.545', '  1403.182', '  1403.818', '  1404.455', '  1405.091', '  1405.727', '  1406.364', '  1407.000', '   ']\n",
      "['  1407.000', '  1408.615', '  1408.615', '  1410.231', '  1411.846', '  1413.462', '  1415.077', '  1416.692', '  1418.308', '  1419.923', '  1421.538', '  1423.154', '  1424.769', '  1426.385', '  1428.000', '   ']\n",
      "['  1428.000', '  1428.636', '  1428.636', '  1429.273', '  1429.909', '  1430.545', '  1431.182', '  1431.818', '  1432.455', '  1433.091', '  1433.727', '  1434.364', '  1435.000', '   ']\n",
      "['  1435.000', '  1436.615', '  1436.615', '  1438.231', '  1439.846', '  1441.462', '  1443.077', '  1444.692', '  1446.308', '  1447.923', '  1449.538', '  1451.154', '  1452.769', '  1454.385', '  1456.000', '   ']\n",
      "['  1456.000', '  1456.636', '  1456.636', '  1457.273', '  1457.909', '  1458.545', '  1459.182', '  1459.818', '  1460.455', '  1461.091', '  1461.727', '  1462.364', '  1463.000', '   ']\n",
      "['  1463.000', '  1464.615', '  1464.615', '  1466.231', '  1467.846', '  1469.462', '  1471.077', '  1472.692', '  1474.308', '  1475.923', '  1477.538', '  1479.154', '  1480.769', '  1482.385', '  1484.000', '   ']\n",
      "['  1484.000', '  1484.636', '  1484.636', '  1485.273', '  1485.909', '  1486.545', '  1487.182', '  1487.818', '  1488.455', '  1489.091', '  1489.727', '  1490.364', '  1491.000', '   ']\n",
      "['  1491.000', '  1492.615', '  1492.615', '  1494.231', '  1495.846', '  1497.462', '  1499.077', '  1500.692', '  1502.308', '  1503.923', '  1505.538', '  1507.154', '  1508.769', '  1510.385', '  1512.000', '   ']\n",
      "['  1512.000', '  1512.636', '  1512.636', '  1513.273', '  1513.909', '  1514.545', '  1515.182', '  1515.818', '  1516.455', '  1517.091', '  1517.727', '  1518.364', '  1519.000', '   ']\n",
      "['  1519.000', '  1520.615', '  1520.615', '  1522.231', '  1523.846', '  1525.462', '  1527.077', '  1528.692', '  1530.308', '  1531.923', '  1533.538', '  1535.154', '  1536.769', '  1538.385', '  1540.000', '   ']\n",
      "['  1540.000', '  1540.636', '  1540.636', '  1541.273', '  1541.909', '  1542.545', '  1543.182', '  1543.818', '  1544.455', '  1545.091', '  1545.727', '  1546.364', '  1547.000', '   ']\n",
      "['  1547.000', '  1548.615', '  1548.615', '  1550.231', '  1551.846', '  1553.462', '  1555.077', '  1556.692', '  1558.308', '  1559.923', '  1561.538', '  1563.154', '  1564.769', '  1566.385', '  1568.000', '   ']\n",
      "['  1568.000', '  1568.636', '  1568.636', '  1569.273', '  1569.909', '  1570.545', '  1571.182', '  1571.818', '  1572.455', '  1573.091', '  1573.727', '  1574.364', '  1575.000', '   ']\n",
      "['  1575.000', '  1576.615', '  1576.615', '  1578.231', '  1579.846', '  1581.462', '  1583.077', '  1584.692', '  1586.308', '  1587.923', '  1589.538', '  1591.154', '  1592.769', '  1594.385', '  1596.000', '   ']\n",
      "['  1596.000', '  1596.636', '  1596.636', '  1597.273', '  1597.909', '  1598.545', '  1599.182', '  1599.818', '  1600.455', '  1601.091', '  1601.727', '  1602.364', '  1603.000', '   ']\n",
      "['  1603.000', '  1604.615', '  1604.615', '  1606.231', '  1607.846', '  1609.462', '  1611.077', '  1612.692', '  1614.308', '  1615.923', '  1617.538', '  1619.154', '  1620.769', '  1622.385', '  1624.000', '   ']\n",
      "['  1624.000', '  1624.636', '  1624.636', '  1625.273', '  1625.909', '  1626.545', '  1627.182', '  1627.818', '  1628.455', '  1629.091', '  1629.727', '  1630.364', '  1631.000', '   ']\n",
      "['  1631.000', '  1632.615', '  1632.615', '  1634.231', '  1635.846', '  1637.462', '  1639.077', '  1640.692', '  1642.308', '  1643.923', '  1645.538', '  1647.154', '  1648.769', '  1650.385', '  1652.000', '   ']\n",
      "['  1652.000', '  1652.636', '  1652.636', '  1653.273', '  1653.909', '  1654.545', '  1655.182', '  1655.818', '  1656.455', '  1657.091', '  1657.727', '  1658.364', '  1659.000', '   ']\n",
      "['  1659.000', '  1660.615', '  1660.615', '  1662.231', '  1663.846', '  1665.462', '  1667.077', '  1668.692', '  1670.308', '  1671.923', '  1673.538', '  1675.154', '  1676.769', '  1678.385', '  1680.000', '   ']\n",
      "['  1680.000', '  1680.636', '  1680.636', '  1681.273', '  1681.909', '  1682.545', '  1683.182', '  1683.818', '  1684.455', '  1685.091', '  1685.727', '  1686.364', '  1687.000', '   ']\n",
      "['  1687.000', '  1688.615', '  1688.615', '  1690.231', '  1691.846', '  1693.462', '  1695.077', '  1696.692', '  1698.308', '  1699.923', '  1701.538', '  1703.154', '  1704.769', '  1706.385', '  1708.000', '   ']\n",
      "['  1708.000', '  1708.636', '  1708.636', '  1709.273', '  1709.909', '  1710.545', '  1711.182', '  1711.818', '  1712.455', '  1713.091', '  1713.727', '  1714.364', '  1715.000', '   ']\n",
      "['  1715.000', '  1716.615', '  1716.615', '  1718.231', '  1719.846', '  1721.462', '  1723.077', '  1724.692', '  1726.308', '  1727.923', '  1729.538', '  1731.154', '  1732.769', '  1734.385', '  1736.000', '   ']\n",
      "['  1736.000', '  1736.636', '  1736.636', '  1737.273', '  1737.909', '  1738.545', '  1739.182', '  1739.818', '  1740.455', '  1741.091', '  1741.727', '  1742.364', '  1743.000', '   ']\n",
      "['  1743.000', '  1744.615', '  1744.615', '  1746.231', '  1747.846', '  1749.462', '  1751.077', '  1752.692', '  1754.308', '  1755.923', '  1757.538', '  1759.154', '  1760.769', '  1762.385', '  1764.000', '   ']\n",
      "['  1764.000', '  1764.636', '  1764.636', '  1765.273', '  1765.909', '  1766.545', '  1767.182', '  1767.818', '  1768.455', '  1769.091', '  1769.727', '  1770.364', '  1771.000', '   ']\n",
      "['  1771.000', '  1772.615', '  1772.615', '  1774.231', '  1775.846', '  1777.462', '  1779.077', '  1780.692', '  1782.308', '  1783.923', '  1785.538', '  1787.154', '  1788.769', '  1790.385', '  1792.000', '   ']\n",
      "['  1792.000', '  1792.636', '  1792.636', '  1793.273', '  1793.909', '  1794.545', '  1795.182', '  1795.818', '  1796.455', '  1797.091', '  1797.727', '  1798.364', '  1799.000', '   ']\n",
      "['  1799.000', '  1800.615', '  1800.615', '  1802.231', '  1803.846', '  1805.462', '  1807.077', '  1808.692', '  1810.308', '  1811.923', '  1813.538', '  1815.154', '  1816.769', '  1818.385', '  1820.000', '   ']\n",
      "['  1820.000', '  1820.636', '  1820.636', '  1821.273', '  1821.909', '  1822.545', '  1823.182', '  1823.818', '  1824.455', '  1825.091', '  1825.727', '  1826.364', '  1827.000', '   ']\n",
      "['  1827.000', '  1827.038', '  1827.038', '  1827.077', '  1827.115', '  1827.154', '  1827.192', '  1827.231', '  1827.269', '  1827.308', '  1827.346', '  1827.385', '  1827.423', '  1827.462', '  1827.500', '   ']\n",
      "[(0.0, 7.0), (7.0, 28.0), (28.0, 35.0), (35.0, 56.0), (56.0, 63.0), (63.0, 84.0), (84.0, 91.0), (91.0, 112.0), (112.0, 119.0), (119.0, 140.0), (140.0, 147.0), (147.0, 168.0), (168.0, 175.0), (175.0, 196.0), (196.0, 203.0), (203.0, 224.0), (224.0, 231.0), (231.0, 252.0), (252.0, 259.0), (259.0, 280.0), (280.0, 287.0), (287.0, 308.0), (308.0, 315.0), (315.0, 336.0), (336.0, 343.0), (343.0, 364.0), (364.0, 371.0), (371.0, 392.0), (392.0, 399.0), (399.0, 420.0), (420.0, 427.0), (427.0, 448.0), (448.0, 455.0), (455.0, 476.0), (476.0, 483.0), (483.0, 504.0), (504.0, 511.0), (511.0, 532.0), (532.0, 539.0), (539.0, 560.0), (560.0, 567.0), (567.0, 588.0), (588.0, 595.0), (595.0, 616.0), (616.0, 623.0), (623.0, 644.0), (644.0, 651.0), (651.0, 672.0), (672.0, 679.0), (679.0, 700.0), (700.0, 707.0), (707.0, 728.0), (728.0, 735.0), (735.0, 756.0), (756.0, 763.0), (763.0, 784.0), (784.0, 791.0), (791.0, 812.0), (812.0, 819.0), (819.0, 840.0), (840.0, 847.0), (847.0, 868.0), (868.0, 875.0), (875.0, 896.0), (896.0, 903.0), (903.0, 924.0), (924.0, 931.0), (931.0, 952.0), (952.0, 959.0), (959.0, 980.0), (980.0, 987.0), (987.0, 1008.0), (1008.0, 1015.0), (1015.0, 1036.0), (1036.0, 1043.0), (1043.0, 1064.0), (1064.0, 1071.0), (1071.0, 1092.0), (1092.0, 1099.0), (1099.0, 1120.0), (1120.0, 1127.0), (1127.0, 1148.0), (1148.0, 1155.0), (1155.0, 1176.0), (1176.0, 1183.0), (1183.0, 1204.0), (1204.0, 1211.0), (1211.0, 1232.0), (1232.0, 1239.0), (1239.0, 1260.0), (1260.0, 1267.0), (1267.0, 1288.0), (1288.0, 1295.0), (1295.0, 1316.0), (1316.0, 1323.0), (1323.0, 1344.0), (1344.0, 1351.0), (1351.0, 1372.0), (1372.0, 1379.0), (1379.0, 1400.0), (1400.0, 1407.0), (1407.0, 1428.0), (1428.0, 1435.0), (1435.0, 1456.0), (1456.0, 1463.0), (1463.0, 1484.0), (1484.0, 1491.0), (1491.0, 1512.0), (1512.0, 1519.0), (1519.0, 1540.0), (1540.0, 1547.0), (1547.0, 1568.0), (1568.0, 1575.0), (1575.0, 1596.0), (1596.0, 1603.0), (1603.0, 1624.0), (1624.0, 1631.0), (1631.0, 1652.0), (1652.0, 1659.0), (1659.0, 1680.0), (1680.0, 1687.0), (1687.0, 1708.0), (1708.0, 1715.0), (1715.0, 1736.0), (1736.0, 1743.0), (1743.0, 1764.0), (1764.0, 1771.0), (1771.0, 1792.0), (1792.0, 1799.0), (1799.0, 1820.0), (1820.0, 1827.0), (1827.0, 1827.5)]\n"
     ]
    }
   ],
   "source": [
    "groups = re.split(r'in\\ curies\\ for\\ case\\ \\Sirrad\\S|in\\ curies\\ for\\ case\\ \\Sdecay\\S', \\\n",
    "                  first_file_string, flags=re.MULTILINE)\n",
    "groups = groups[1:]\n",
    "    # The first group is all of the junk before the first actual table.\n",
    "\n",
    "all_times = list()\n",
    "\n",
    "All_LE = pd.Series()\n",
    "All_AC = pd.Series()\n",
    "All_FP = pd.Series()\n",
    "\n",
    "header_unit_list = list()\n",
    "\n",
    "Start_End_Times = list()\n",
    "\n",
    "print(\"Creating DataFrames from\")\n",
    "for number, case in enumerate(groups):\n",
    "    #Used for debugging\n",
    "#         if number > 5:\n",
    "#             break\n",
    "    print(\"Case ({}/{})\\r\".format(number+1,len(groups)), end = '')\n",
    "\n",
    "    # Handle the last table from each split section\n",
    "    #################################\n",
    "\n",
    "    data_from_case = re.sub(r'\\s{2,}(?=\\d+)',\"  \", case)\n",
    "    data_from_case = re.sub(r'^[.]\\s*|^\\s{2}',\"\", data_from_case, flags = re.MULTILINE)\n",
    "    data_from_case = re.split(r'he-3', data_from_case)\n",
    "#     print(data_from_case[0])\n",
    "    header_unit_string = re.findall('\\d{1,}[a-z]{1,2}\\s{2}',data_from_case[0], flags = re.MULTILINE)\n",
    "    header_unit = re.findall('[a-z]{2}|[a-z]{1}', header_unit_string[1])[0]\n",
    "#     print(header_unit)\n",
    "    header_unit_list.append(header_unit)\n",
    "    \n",
    "    header_handling = re.split(r\"{}(.+)\".format(header_unit),data_from_case[0], flags = re.MULTILINE)\n",
    "    \n",
    "    times = header_handling[1].split('{}'.format(header_unit))\n",
    "#     print(times)\n",
    "    start_time = times[1] \n",
    "    times.insert(1,start_time)\n",
    "    Start_End_Times.append((float(times[0]),float(times[-2])))\n",
    "#     print(times)\n",
    "print(Start_End_Times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 't_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-136-c99933f0297a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m#         print(len(Calculated_Times))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# print(len(All_Times), All_Times)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mt_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 't_df' is not defined"
     ]
    }
   ],
   "source": [
    "All_Times = list()\n",
    "for Times, Interval in zip(Start_End_Times, Integer_Interval_List):\n",
    "    Calculated_Times = np.linspace(Times[0],Times[1],2+Interval)[:-1]\n",
    "    for time in Calculated_Times:\n",
    "        All_Times.append(time)\n",
    "All_Times.append(Times[1])\n",
    "#         print(len(Calculated_Times))\n",
    "# print(len(All_Times), All_Times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating DataFrames from\n",
      "Case (1/132)\r"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'time_units' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-86-ca0a2b20d559>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[0mtimes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstart_time\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0mtime_units\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'years'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m         \u001b[0mtimes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m365.25\u001b[0m \u001b[1;31m#The first and last objects are empty strings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'time_units' is not defined"
     ]
    }
   ],
   "source": [
    "# with open(filename, 'r') as file:\n",
    "#     first_file_string = file.read()#.replace('\\n', '')\n",
    "#     file.close()\n",
    "\n",
    "# first_file_string = re.sub(r'(\\d{1}.\\d{4})(-\\d{3})', r'\\1E\\2', first_file_string)\n",
    "#The above searches for \"#.####-###\" and replaces them with \"#.####E-###\"\n",
    "\n",
    "#Due to low significant figures in the output file, we will create our own timestamps using\n",
    "#linearly spaced times using the amount of intervals as defined in the input file which, conveniently\n",
    "#is located in the output files.\n",
    "Case_Intervals = re.findall(r't=\\[[^\\]]*\\]', first_file_string) #Only instance of t=[******], nice\n",
    "\n",
    "Integer_Interval_List = list()\n",
    "\n",
    "for Interval in Case_Intervals:\n",
    "    if 'i' in Interval:\n",
    "        Integer_Interval_List.append(int(re.split('\\[|i',Interval)[1])) #Get the number just before the i\n",
    "            #where i is the amount of intervals. \n",
    "\n",
    "###################################\n",
    "# Split them using key phrase, \"in curies for case {decay/irrad}\"\n",
    "###################################\n",
    "groups = re.split(r'in\\ curies\\ for\\ case\\ \\Sirrad\\S|in\\ curies\\ for\\ case\\ \\Sdecay\\S', \\\n",
    "                  first_file_string, flags=re.MULTILINE)\n",
    "\n",
    "groups = groups[1:]\n",
    "# The first group is all of the junk before the first actual table.\n",
    "\n",
    "all_times = list()\n",
    "\n",
    "All_LE = pd.Series()\n",
    "All_AC = pd.Series()\n",
    "All_FP = pd.Series()\n",
    "\n",
    "print(\"Creating DataFrames from\")\n",
    "for number, case in enumerate(groups):\n",
    "    #Used for debugging\n",
    "#         if number > 5:\n",
    "#             break\n",
    "    print(\"Case ({}/{})\\r\".format(number+1,len(groups)), end = '')\n",
    "\n",
    "    # Handle the last table from each split section\n",
    "    #################################\n",
    "\n",
    "    data_from_case = re.sub(r'\\s{2,}(?=\\d+)',\"  \", case)\n",
    "    data_from_case = re.sub(r'^[.]\\s*|^\\s{2}',\"\", data_from_case, flags = re.MULTILINE)\n",
    "    data_from_case = re.split(r'he-3', data_from_case)\n",
    "    header_handling = re.split(r\"d(.+)\",data_from_case[0], flags = re.MULTILINE) #Splitting the first \n",
    "                                                                #portion of list by the the first occuring d\n",
    "    #################################\n",
    "    times = header_handling[1].split('d') #Splitting string to list for numpy to use\n",
    "#     print(times)\n",
    "    start_time = times[1] #We have two cases of the first time. The index list must be the right length, so we append.\n",
    "                        #This is an artifact of using the first 'd' to split our header\n",
    "    times.insert(1,start_time)\n",
    "\n",
    "    if time_units == 'years':\n",
    "        times = np.array(times[1:-1]).astype(float)/365.25 #The first and last objects are empty strings\n",
    "    else:\n",
    "        times = np.array(times[1:-1]).astype(float)\n",
    "\n",
    "    all_times.append(times)\n",
    "\n",
    "    temp_data = header_handling[2].split('\\n') \n",
    "\n",
    "    #t_df is short for temporary DataFrame. I abbreviate to make it a little bit easier to read\n",
    "\n",
    "    t_df = pd.DataFrame(temp_data)\n",
    "    t_df = t_df[0].str.split(\" * \", expand = True)\n",
    "    t_df.iloc[0] = t_df.iloc[0].shift(1) #This gets rid of the first column. \n",
    "    t_df = t_df.transpose() # We will be transposing all of our dataframes.\n",
    "                # We want to build on times Not on isotopes.\n",
    "    header = t_df.iloc[0] #The first row in this dataframe is the isotope names we want\n",
    "    for number, head in enumerate(header):\n",
    "        if number > 0:\n",
    "            header[number] = re.sub(r'\\-', \"\", head) #Get rid of the dash in the isotope names\n",
    "\n",
    "    ##################################################################################################################\n",
    "\n",
    "    temp_data = data_from_case[1].split('\\n')\n",
    "    t_df = pd.DataFrame(temp_data)\n",
    "    t_df = t_df[0].str.split(\" * \", expand = True)\n",
    "    t_df.iloc[0] = t_df.iloc[0].shift(1)\n",
    "    t_df = t_df.transpose()\n",
    "    header = t_df.iloc[0]\n",
    "    for number, head in enumerate(header):\n",
    "        if number > 0:\n",
    "            header[number] = re.sub(r'\\-', \"\", head)\n",
    "\n",
    "    header = pd.concat([pd.Series([\"he3\"]),header[1:]]) #Adding he3 as the name back to its respective isotope\n",
    "\n",
    "    #Here we create a mask in order to name columns and index easier (maybe not easier but I like it this way)\n",
    "    df_naming_mask = t_df[2:] #Get rid of those pesky empty lines\n",
    "    df_naming_mask.columns = header #Name the columns by their isotope partner's name\n",
    "    LE_df_final = df_naming_mask.set_index(times)\n",
    "\n",
    "    ##################################################################################################################\n",
    "    #See above for what we're doing here\n",
    "    temp_data = data_from_case[2].split('\\n')\n",
    "    t_df = pd.DataFrame(temp_data)\n",
    "    t_df = t_df[0].str.split(\" * \", expand = True)\n",
    "    t_df.iloc[0] = t_df.iloc[0].shift(1)\n",
    "    t_df = t_df.transpose()\n",
    "    header = t_df.iloc[0]\n",
    "    for number, head in enumerate(header):\n",
    "        if number > 0:\n",
    "            header[number] = re.sub(r'\\-', \"\", head)\n",
    "\n",
    "\n",
    "    header = pd.concat([pd.Series([\"he3\"]),header[1:]])\n",
    "    df_naming_mask = t_df[2:]\n",
    "    df_naming_mask.columns = header\n",
    "    AC_df_final = df_naming_mask.set_index(times)\n",
    "\n",
    "    ##################################################################################################################\n",
    "    # Handle the last table from each split section\n",
    "    FP_handling = re.split(r\"\\-{6,}\",data_from_case[3]) #Gets rid of everything after the actual .out file\n",
    "\n",
    "    #Back to what we've seen before (if you've been paying attention ;)\n",
    "    temp_data = FP_handling[0].split('\\n')\n",
    "    t_df = pd.DataFrame(temp_data)\n",
    "    t_df = t_df[0].str.split(\" * \", expand = True)\n",
    "    t_df.iloc[0] = t_df.iloc[0].shift(1)\n",
    "    t_df = t_df.transpose()\n",
    "    header = t_df.iloc[0]\n",
    "    for number, head in enumerate(header):\n",
    "        if number > 0:\n",
    "            header[number] = re.sub(r'\\-', \"\", head)\n",
    "    header = pd.concat([pd.Series([\"he3\"]),header[1:]])\n",
    "    df_naming_mask = t_df[2:]\n",
    "    df_naming_mask.columns = header\n",
    "#         if number == 0:\n",
    "#             df_naming_mask = t_df\n",
    "#             df_naming_mask.columns = header\n",
    "#             FP_df_final = df_naming_mask.set_index(times)\n",
    "#         else:\n",
    "#             df_naming_mask = t_df[2:]\n",
    "#             df_naming_mask.columns = header\n",
    "    FP_df_final = df_naming_mask.set_index(times).iloc[1:] #This is very close to doing what I want\n",
    "    print(FP_df_final.index[0] ,FP_df_final.index[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# Chad Denbrock\n",
    "# Niowave Inc.\n",
    "# Produced: 03.04.2020\n",
    "# Last updated: 08.06.2020\n",
    "################################################################################\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from openpyxl import Workbook\n",
    "from openpyxl import load_workbook\n",
    "import matplotlib as mpl\n",
    "import re\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "Time = time.time()\n",
    "mpl.rcParams['savefig.dpi']  = 500\n",
    "mpl.rcParams['font.size']    = 12\n",
    "mpl.rcParams['mathtext.fontset'] = 'stix'\n",
    "mpl.rcParams['font.family'] = 'STIXGeneral'\n",
    "\n",
    "\n",
    "################################################################################\n",
    "# Goal: Plot a fission product/actinide inventory from UTA-2 for a given\n",
    "#           irradiation/decay cycle.\n",
    "#\n",
    "#\n",
    "# Requirements:\n",
    "#       Two excel (.xlsx) files. One for each the NU and LEU portions of UTA-2.\n",
    "#           They must be named with 'NU' and 'LEU' being the beginning characters\n",
    "#           in the file like 'NU blah blah.xlsx' and 'LEU blah blah.xlsx'. These\n",
    "#           excel files must be copy and pasted acitivities as a function of time\n",
    "#           from F71 output files from Origen. This includes the isotope names\n",
    "#           being the column names with the activities as a function of time\n",
    "#           progressing in the rows in the column. The irradiation/decay cycle\n",
    "#           for the NU and LEU obviously must be the same for the two excel\n",
    "#           documents. The NU and LEU inventories will inveitably have different\n",
    "#           isotopes in their inventory. This is not a problem.\n",
    "#       The 'Half_Lives_List.txt' half-life library used to check whether the\n",
    "#           isotopes have a half-life less than or greater than or equal to\n",
    "#           120 days.\n",
    "################################################################################\n",
    "\n",
    "#   No 100 gNU MCNP calculations were performed for the optimized UTA-2 configuration\n",
    "# def convert_to_100gNU(NU) :\n",
    "#\n",
    "#     fraction_FP_and_An_in_100gNU_over_total_NU = 0.01743\n",
    "#     fraction_U237_in_100gNU_over_total_NU = 0.03485\n",
    "#\n",
    "#     Time = NU['Unnamed: 0']\n",
    "#     NU = NU.drop(columns = 'Unnamed: 0')\n",
    "#     Updated_Inventory = NU*fraction_FP_and_An_in_100gNU_over_total_NU\n",
    "#     if kind_of_isotopes.lower() == 'actinides' or kind_of_isotopes.lower() == 'an' :\n",
    "#         Updated_Inventory['u237'] = Updated_Inventory['u237']*fraction_U237_in_100gNU_over_total_NU/fraction_FP_and_An_in_100gNU_over_total_NU\n",
    "#     Updated_Inventory = pd.DataFrame.sort_values(Updated_Inventory,Updated_Inventory.shape[0]-1,axis=1,ascending=False)\n",
    "#     Updated_Inventory.insert(loc=0,column = f'Time ({time_units})',value = Time)\n",
    "#\n",
    "#     print('Writing updated inventory to Inventory_Hottest_100gNU.xlsx ...\\n')\n",
    "#\n",
    "#     while True :\n",
    "#         try :\n",
    "#             Updated_Inventory.to_excel('Inventory_Hottest_100gNU.xlsx')\n",
    "#             break\n",
    "#         except :\n",
    "#             print('There was a problem with writing Inventory_Hottest_100gNU.xlsx. '\\\n",
    "#                 'It may already exist. Delete it if it does. The script will try to '\\\n",
    "#                 'write it again in 15 seconds.\\n')\n",
    "#             time.sleep(15)\n",
    "#     return Updated_Inventory\n",
    "\n",
    "def convert_to_1kgNU(NU, time_units) :\n",
    "\n",
    "    Mass_per_rod_gU = 134.6\n",
    "    Rods_per_1kgNU = 1000/Mass_per_rod_gU\n",
    "    scaling = Rods_per_1kgNU/7\n",
    "\n",
    "\n",
    "    fraction_FP_and_An_in_7rods_over_total_NU = 0.1187849\n",
    "    fraction_U237_in_7rods_over_total_NU = 0.3376\n",
    "\n",
    "    fraction_FP_and_An_in_1kgNU_over_total_NU = fraction_FP_and_An_in_7rods_over_total_NU * scaling     # Linearly scaling fractions from 7 rods to ~ 7.429 rods in 1 kgNU\n",
    "    fraction_U237_in_1kgNU_over_total_NU = fraction_U237_in_7rods_over_total_NU * scaling               # Linearly scaling fractions from 7 rods to ~ 7.429 rods in 1 kgNU\n",
    "\n",
    "    Time = NU['Time ({})'.format(time_units)]\n",
    "    NU = NU.drop(columns = 'Time ({})'.format(time_units))\n",
    "    Updated_Inventory = NU*fraction_FP_and_An_in_1kgNU_over_total_NU\n",
    "\n",
    "    if kind_of_isotopes.lower() == 'actinides' or kind_of_isotopes.lower() == 'an' :\n",
    "        Updated_Inventory['u237'] = Updated_Inventory['u237']*fraction_U237_in_1kgNU_over_total_NU/fraction_FP_and_An_in_1kgNU_over_total_NU\n",
    "#     Updated_Inventory = pd.DataFrame.sort_values(Updated_Inventory,Updated_Inventory.shape[0]-1,axis=1,ascending=False)\n",
    "#     Updated_Inventory[\"Total_Activity\"] = Updated_Inventory.iloc[:, 1:].sum(axis = 1)\n",
    "#     print(\"AHH\")\n",
    "#     print(Updated_Inventory['Total_Activity'])\n",
    "#     Updated_Inventory = Updated_Inventory.sort_values(Updated_Inventory.index[Updated_Inventory.shape[0]-1], axis = 1, ascending = False) #Added by Austin to replace the above commented out line\n",
    "#     Updated_Inventory = Updated_Inventory.sort_values(Updated_Inventory.iloc[-1], axis = 0, ascending = False) #Added by Austin to replace the above commented out line\n",
    "    Updated_Inventory = Updated_Inventory.sort_values(Updated_Inventory.iloc[-1].name, axis = 1, ascending = False)\n",
    "    Updated_Inventory.insert(loc=0,column = f'Time ({time_units})',value = Time)\n",
    "\n",
    "    print('Writing updated inventory to The_Hottest_1kgNU_Inventory.xlsx ...')\n",
    "    print(\"This may take a few minutes ...\\n\")\n",
    "    while True :\n",
    "        try :\n",
    "            Updated_Inventory.to_excel('The_Hottest_1kgNU_Inventory.xlsx')\n",
    "            break\n",
    "        except :\n",
    "            print('There was a problem with writing The_Hottest_1kgNU_Inventory.xlsx. '\\\n",
    "                'It may already exist. Delete it if it does. The script will try to '\\\n",
    "                'write it again in 15 seconds.\\n')\n",
    "            time.sleep(15)\n",
    "    return Updated_Inventory\n",
    "\n",
    "def U_237_adder(NU,LEU, time_units) :\n",
    "\n",
    "    fraction_U_237_in_LEU = 0.737\n",
    "    fraction_U_237_in_NU = 1.0 - fraction_U_237_in_LEU\n",
    "    Total_core_reaction_rate_per_e = 2.386e-4     # Total U-237 reactions/e for 20 MeV electrons in traditional UTA-2 geometry\n",
    "    Electron_rate = 2.184e15\n",
    "    print(f'The total core reaction rate per electron for U-237 production '\\\n",
    "        f'{Total_core_reaction_rate_per_e:.3e} and the electron source intensity '\\\n",
    "        f'{Electron_rate:.3e} are hardcoded in for a 20 MeV electron beam and need to be changed for alternate '\\\n",
    "        'configurations of UTA-2.\\n'\\\n",
    "        'The neutron flux to achieve the ORIGEN results and the electron rate to '\\\n",
    "        'achieve the U-237 production are no longer physically linked. So, be '\\\n",
    "        'careful and make sure the hardcoded electron rate corresponds to the '\\\n",
    "        'electron rate necessary to produce the fission power input into ORIGEN.\\n\\n')\n",
    "\n",
    "    Total_core_reaction_rate = Electron_rate * Total_core_reaction_rate_per_e\n",
    "\n",
    "\n",
    "    half_life_U_237 = 6.752*86400\n",
    "    lambda_U_237 = np.log(2)/half_life_U_237\n",
    "    try :\n",
    "        test_activity = pd.Series.to_numpy(LEU['np239'],dtype = 'float')\n",
    "    except :\n",
    "        print('Np239 doesnt exist in the LEU spreadsheet. This probably means you incorrectly asked for what youre plotting.\\n')\n",
    "        exit()\n",
    "    Time = pd.Series.to_numpy(NU['Time ({})'.format(time_units)],dtype = 'float')\n",
    "\n",
    "#     if time_units.lower() == 'years' :\n",
    "#         dt_multiplier = 86400*365.25\n",
    "\n",
    "#     elif time_units.lower() == 'days' :\n",
    "#         dt_multiplier = 86400.0\n",
    "    if time_units == 'years' :\n",
    "        dt_multiplier = 86400*365.25\n",
    "\n",
    "    elif time_units == 'days' :\n",
    "        dt_multiplier = 86400.0\n",
    "\n",
    "    U_237 = np.zeros(len(Time))\n",
    "\n",
    "\n",
    "    growth_guess = 0\n",
    "    first_guess_completed = False\n",
    "\n",
    "    for time_values in range(len(Time)-1) :\n",
    "\n",
    "\n",
    "        if test_activity[time_values] == 0 or (test_activity[time_values+1] - test_activity[time_values])/test_activity[time_values] >= growth_guess:\n",
    "\n",
    "            dt = (Time[time_values + 1] - Time[time_values])*dt_multiplier\n",
    "            dU_237 = (Total_core_reaction_rate - lambda_U_237*U_237[time_values])*dt\n",
    "\n",
    "        else :\n",
    "\n",
    "            if first_guess_completed == False :\n",
    "                growth_guess = (test_activity[time_values-1] - test_activity[time_values])/(50*test_activity[time_values-1])\n",
    "                first_guess_completed = True\n",
    "\n",
    "\n",
    "            dt = (Time[time_values + 1] - Time[time_values])*dt_multiplier\n",
    "            dU_237 =                           - lambda_U_237*U_237[time_values]*dt\n",
    "\n",
    "#         if time_units == 'Years' and (Time[time_values] == 4.006471 or Time[time_values] ==4.004729):\n",
    "        if time_units == 'Years' and (Time[time_values] == 4.006471 or Time[time_values] ==4.004729):\n",
    "            #print('\\nin here\\n')\n",
    "            dU_237 = 0.0\n",
    "        #print(Time[time_values],test_activity[time_values+1] - test_activity[time_values],dU_237,U_237[time_values],dt)\n",
    "\n",
    "        U_237[time_values+1] = U_237[time_values] + dU_237\n",
    "    U_237 = U_237 * lambda_U_237/3.7e10\n",
    "\n",
    "    #plt.plot(Time,U_237)\n",
    "    #plt.plot(Time,U_237*fraction_U_237_in_NU)\n",
    "    #plt.plot(Time,U_237*fraction_U_237_in_LEU)\n",
    "    #plt.show()\n",
    "\n",
    "    if 'u237' not in NU.columns :\n",
    "        NU.insert(loc = 0,column = 'u237',value=U_237*fraction_U_237_in_NU)\n",
    "    else :\n",
    "        NU = NU.add(pd.DataFrame(U_237*fraction_U_237_in_NU,columns = ['u237']),fill_value = 0)\n",
    "\n",
    "    if 'u237' not in LEU.columns :\n",
    "        LEU.insert(loc = 0,column = 'u237',value=U_237*fraction_U_237_in_LEU)\n",
    "    else :\n",
    "        LEU = LEU.add(pd.DataFrame(U_237*fraction_U_237_in_LEU,columns = ['u237']),fill_value = 0)\n",
    "    return NU,LEU\n",
    "\n",
    "\n",
    "def plotting(The_Inventory) :\n",
    "    eff_list = ['kr85m','kr88','kr85','kr87','i131','i132','i133','i135','xe133','xe133m','xe135']\n",
    "    Input = list()\n",
    "\n",
    "\n",
    "    Splitting = True    # Choosing to always split the inventories for plotting purposes\n",
    "\n",
    "#       Asking for what to plot (totals or specific isotopes)\n",
    "    top_regex = '[Tt]op [0-9]+'\n",
    "    while True :\n",
    "        #try :\n",
    "        Input.append(input('Name isotopes you wish to plot. You can also plot the total by typing \\'Total\\', the top x isotopes by activity at final time by typing \\'Top <integer number of isotopes you want to see>\\', or the effluents list by typing \\'Effluents\\'. (Hit enter after each one and type \\'Stop\\' to quit entering isotopes)\\n'))\n",
    "        if Input[-1].lower() == 'effluents' :\n",
    "            Input = Input[:-1] + eff_list\n",
    "        if len(re.findall(top_regex,Input[-1])) > 0:\n",
    "            top_quantity = int(Input[-1].split()[1])\n",
    "            Input = Input[:-1] + list(The_Inventory.columns[3:top_quantity+3])\n",
    "        if Input[-1].lower() == 'stop' :\n",
    "            Input = Input[:-1]\n",
    "            break\n",
    "        #except :\n",
    "            #print('Try that again. Make sure to hit enter after each entry. The form should be: \\'np239\\'\\n')\n",
    "\n",
    "\n",
    "\n",
    "    Greater_than,Less_than = split_120(The_Inventory)     # Split the inventory by half-lives and retrieve the time vector\n",
    "\n",
    "    Time = The_Inventory[f'Time ({time_units})']\n",
    "\n",
    "    if 'Total' in Input:\n",
    "        # -----------------------\n",
    "        # Plotting both HL's\n",
    "        # -----------------------\n",
    "        plt.plot(Time,Less_than['Total'])\n",
    "        if len(Greater_than['Total']) > 0:\n",
    "            plt.plot(Time,Greater_than['Total'])\n",
    "        legend_list = ['Half Life < 120 d','Half Life > 120 d']\n",
    "        for vals in Input :\n",
    "            if vals != 'Total'  :\n",
    "                legend_list.append(vals)\n",
    "        #plt.show()\n",
    "        if len(Greater_than['Total']) > 0:\n",
    "            max = (Greater_than['Total'] + Less_than['Total']).max()\n",
    "            end = pd.Series.to_numpy(Greater_than['Total'] + Less_than['Total'])[-1]\n",
    "            print(f'\\nThe max total activity was {max:.3f} (Ci)')\n",
    "            print(f'The final total activity was {end:.3f} (Ci)')\n",
    "            max = Greater_than['Total'].max()\n",
    "            end = pd.Series.to_numpy(Greater_than['Total'])[-1]\n",
    "            print(f'The max activity (HL > 120 d) was {max:.3f} (Ci)')\n",
    "            print(f'The final activity (HL > 120 d) was {end:.3f} (Ci)')\n",
    "            max = Less_than['Total'].max()\n",
    "            end = pd.Series.to_numpy(Less_than['Total'])[-1]\n",
    "            print(f'The max activity (HL < 120 d) was {max:.3f} (Ci)')\n",
    "            print(f'The final activity (HL < 120 d) was {end:.3f} (Ci)')\n",
    "        else :\n",
    "            max = Less_than['Total'].max()\n",
    "            end = pd.Series.to_numpy(Less_than['Total'])[-1]\n",
    "            print(f'The max total activity was {max:.3f} (Ci)')\n",
    "            print(f'The final total activity was {end:.3f} (Ci)\\n')\n",
    "        for vals in Input :\n",
    "            if vals != 'Total'  :\n",
    "\n",
    "                plt.plot(Time,pd.Series.to_numpy(The_Inventory[vals]))\n",
    "                print(f'The max activity of {vals} was {pd.Series.to_numpy(The_Inventory[vals]).max():.3f} (Ci)')\n",
    "                print(f'The final activity of {vals} was {pd.Series.to_numpy(The_Inventory[vals])[-1]:.3f} (Ci)')\n",
    "        plt.grid(which = 'both', axis = 'both')\n",
    "        plt.legend(legend_list)\n",
    "        plt.title(f'Activity of {Title}')\n",
    "        plt.xlabel(time_units)\n",
    "        plt.ylabel('Activity (Ci)')\n",
    "        plt.savefig(fname + '_Both.png',bbox_inches = 'tight')\n",
    "        print(f'\\nFigure {fname}_Both.png produced and saved.\\n')\n",
    "        plt.close()\n",
    "        # -----------------------\n",
    "        # Plotting HL >\n",
    "        # -----------------------\n",
    "        if len(Greater_than['Total']) > 0:\n",
    "            plt.plot(Time,Greater_than['Total'])\n",
    "            legend_list = ['Half Life > 120 d']\n",
    "            for vals in Input :\n",
    "                if vals != 'Total'  :\n",
    "                    legend_list.append(vals)\n",
    "                    plt.plot(Time,pd.Series.to_numpy(The_Inventory[vals]))\n",
    "            plt.grid(which = 'both', axis = 'both')\n",
    "            plt.legend(legend_list)\n",
    "            plt.title(f'Activity of {Title} (HL > 120 days)')\n",
    "            plt.xlabel(time_units)\n",
    "            plt.ylabel('Activity (Ci)')\n",
    "            plt.savefig(fname + '_Greaterthan.png',bbox_inches = 'tight')\n",
    "            print(f'Figure {fname}_Greaterthan.png produced and saved.\\n')\n",
    "            plt.close()\n",
    "        # -----------------------\n",
    "        # Plotting HL <\n",
    "        # -----------------------\n",
    "        plt.plot(Time,Less_than['Total'])\n",
    "        legend_list = ['Half Life < 120 d']\n",
    "        for vals in Input :\n",
    "            if vals != 'Total'  :\n",
    "                plt.plot(Time,pd.Series.to_numpy(The_Inventory[vals]))\n",
    "                legend_list.append(vals)\n",
    "        plt.grid(which = 'both', axis = 'both')\n",
    "        plt.legend(legend_list)\n",
    "        plt.title(f'Activity of {Title} (HL < 120 days)')\n",
    "        plt.xlabel(time_units)\n",
    "        plt.ylabel('Activity (Ci)')\n",
    "        plt.savefig(fname + '_Lessthan.png',bbox_inches = 'tight')\n",
    "        print(f'Figure {fname}_Lessthan.png produced and saved.\\n')\n",
    "        plt.close()\n",
    "        # -----------------------\n",
    "        # Plotting Individual Isotopes (doesnt work)\n",
    "        # -----------------------\n",
    "        # for vals in Input :\n",
    "        #     if vals != 'Total'  :\n",
    "        #         plt.plot(Time,pd.Series.to_numpy(The_Inventory[vals]))\n",
    "        #         legend_list.append(vals)\n",
    "        # plt.grid(which = 'both', axis = 'both')\n",
    "        # plt.legend(legend_list)\n",
    "        # plt.title(f'Activity of {Title} (HL < 120 days)')\n",
    "        # plt.xlabel(time_units)\n",
    "        # plt.ylabel('Activity (Ci)')\n",
    "        # plt.savefig(fname + 'Less.png',bbox_inches = 'tight')\n",
    "        # print(f'Figure {fname}Less.png produced and saved.\\n')\n",
    "        # plt.close()\n",
    "\n",
    "    else :\n",
    "        legend_list = list()\n",
    "        for vals in Input :\n",
    "            try :\n",
    "                if vals != 'Total'  :\n",
    "                    plt.plot(Time,pd.Series.to_numpy(The_Inventory[vals]),linewidth=0.75)\n",
    "                    print(f'The max activity of {vals} was {pd.Series.to_numpy(The_Inventory[vals]).max():.3f} (Ci)')\n",
    "                    print(f'The final activity of {vals} was {pd.Series.to_numpy(The_Inventory[vals])[-1]:.3f} (Ci)')\n",
    "                    legend_list.append(vals)\n",
    "            except :\n",
    "                print(f'\\nPlotting {vals} wasnt found. Check if it is in the isotope list and try again.\\n')\n",
    "                continue\n",
    "\n",
    "        if len(legend_list)>0 :\n",
    "            plt.grid(which = 'both', axis = 'both')\n",
    "            plt.legend(legend_list)\n",
    "            plt.title(f'Activity of {Title}')\n",
    "            plt.xlabel(time_units)\n",
    "            plt.ylabel('Activity (Ci)')\n",
    "            plt.savefig(fname,bbox_inches = 'tight')\n",
    "            print(f'\\nFigure {fname} produced and saved.\\n')\n",
    "            plt.close()\n",
    "\n",
    "\n",
    "def output_reader(filename, NOI, time_units = 'days'):\n",
    "    \"\"\"\n",
    "    Austin Czyzewski's function to read values from ORIGEN .out files\n",
    "    The workflow of this function goes something like this:\n",
    "        - Import a file as one large string\n",
    "        - Seperate string into Cases in .out file\n",
    "        - Create a dataframe from the actual cases themselves\n",
    "            - Get rid of the information we already know that surrounds the data\n",
    "            - Split each case into its three parts: Light Elements, Fission Products, and Actinides\n",
    "            - Use temporary dataframes that will constantly be overwritten to not destroy our PC\n",
    "        - Append each case to an overall dataframe for each isotope source type (AC, LE, FP)\n",
    "        - Add the index in as a Time column in the dataframe for plotting\n",
    "        - Use pandas built in method to convert from strings to floats\n",
    "    \"\"\"\n",
    "    ###################################\n",
    "    ### Import the file as a string ###\n",
    "    ###################################\n",
    "    with open(filename, 'r') as file:\n",
    "        first_file_string = file.read()#.replace('\\n', '')\n",
    "        file.close()\n",
    "        \n",
    "    first_file_string = re.sub(r'(\\d{1}.\\d{4})(-\\d{3})', r'\\1E\\2', first_file_string)\n",
    "    #The above searches for \"#.####-###\" and replaces them with \"#.####E-###\"\n",
    "    \n",
    "    #Due to low significant figures in the output file, we will create our own timestamps using\n",
    "    #linearly spaced times using the amount of intervals as defined in the input file which, conveniently\n",
    "    #is located in the output files.\n",
    "    Case_Intervals = re.findall(r't=\\[[^\\]]*\\]', first_file_string) #Only instance of t=[******], nice\n",
    "\n",
    "    Integer_Interval_List = list()\n",
    "    \n",
    "    for Interval in Case_Intervals:\n",
    "        if 'i' in Interval:\n",
    "            Integer_Interval_List.append(int(re.split('\\[|i',Interval)[1])) #Get the number just before the i\n",
    "                #where i is the amount of intervals. \n",
    "                \n",
    "    ###################################\n",
    "    # Split them using key phrase, \"in curies for case {decay/irrad}\"\n",
    "    ###################################\n",
    "    groups = re.split(r'in\\ curies\\ for\\ case\\ \\Sirrad\\S|in\\ curies\\ for\\ case\\ \\Sdecay\\S', \\\n",
    "                      first_file_string, flags=re.MULTILINE)\n",
    "    \n",
    "    groups = groups[1:]\n",
    "    # The first group is all of the junk before the first actual table.\n",
    "\n",
    "    all_times = list()\n",
    "\n",
    "    All_LE = pd.Series()\n",
    "    All_AC = pd.Series()\n",
    "    All_FP = pd.Series()\n",
    "    \n",
    "    print(\"Creating DataFrames from\")\n",
    "    for number, case in enumerate(groups):\n",
    "        #Used for debugging\n",
    "#         if number > 5:\n",
    "#             break\n",
    "        print(\"Case ({}/{})\\r\".format(number+1,len(groups)), end = '')\n",
    "        \n",
    "        # Handle the last table from each split section\n",
    "        #################################\n",
    "        \n",
    "        data_from_case = re.sub(r'\\s{2,}(?=\\d+)',\"  \", case)\n",
    "        data_from_case = re.sub(r'^[.]\\s*|^\\s{2}',\"\", data_from_case, flags = re.MULTILINE)\n",
    "        data_from_case = re.split(r'he-3', data_from_case)\n",
    "        header_handling = re.split(r\"d(.+)\",data_from_case[0], flags = re.MULTILINE) #Splitting the first \n",
    "                                                                    #portion of list by the the first occuring d\n",
    "        #################################\n",
    "        times = header_handling[1].split('d') #Splitting string to list for numpy to use\n",
    "    #     print(times)\n",
    "        start_time = times[1] #We have two cases of the first time. The index list must be the right length, so we append.\n",
    "                            #This is an artifact of using the first 'd' to split our header\n",
    "        times.insert(1,start_time)\n",
    "        \n",
    "        if time_units == 'years':\n",
    "            times = np.array(times[1:-1]).astype(float)/365.25 #The first and last objects are empty strings\n",
    "        else:\n",
    "            times = np.array(times[1:-1]).astype(float)\n",
    "            \n",
    "        all_times.append(times)\n",
    "        \n",
    "        temp_data = header_handling[2].split('\\n') \n",
    "        \n",
    "        #t_df is short for temporary DataFrame. I abbreviate to make it a little bit easier to read\n",
    "        \n",
    "        t_df = pd.DataFrame(temp_data)\n",
    "        t_df = t_df[0].str.split(\" * \", expand = True)\n",
    "        t_df.iloc[0] = t_df.iloc[0].shift(1) #This gets rid of the first column. \n",
    "        t_df = t_df.transpose() # We will be transposing all of our dataframes.\n",
    "                    # We want to build on times Not on isotopes.\n",
    "        header = t_df.iloc[0] #The first row in this dataframe is the isotope names we want\n",
    "        for number, head in enumerate(header):\n",
    "            if number > 0:\n",
    "                header[number] = re.sub(r'\\-', \"\", head) #Get rid of the dash in the isotope names\n",
    "\n",
    "        ##################################################################################################################\n",
    "        \n",
    "        temp_data = data_from_case[1].split('\\n')\n",
    "        t_df = pd.DataFrame(temp_data)\n",
    "        t_df = t_df[0].str.split(\" * \", expand = True)\n",
    "        t_df.iloc[0] = t_df.iloc[0].shift(1)\n",
    "        t_df = t_df.transpose()\n",
    "        header = t_df.iloc[0]\n",
    "        for number, head in enumerate(header):\n",
    "            if number > 0:\n",
    "                header[number] = re.sub(r'\\-', \"\", head)\n",
    "            \n",
    "        header = pd.concat([pd.Series([\"he3\"]),header[1:]]) #Adding he3 as the name back to its respective isotope\n",
    "        \n",
    "        #Here we create a mask in order to name columns and index easier (maybe not easier but I like it this way)\n",
    "        df_naming_mask = t_df[2:] #Get rid of those pesky empty lines\n",
    "        df_naming_mask.columns = header #Name the columns by their isotope partner's name\n",
    "        LE_df_final = df_naming_mask.set_index(times)\n",
    "\n",
    "        ##################################################################################################################\n",
    "        #See above for what we're doing here\n",
    "        temp_data = data_from_case[2].split('\\n')\n",
    "        t_df = pd.DataFrame(temp_data)\n",
    "        t_df = t_df[0].str.split(\" * \", expand = True)\n",
    "        t_df.iloc[0] = t_df.iloc[0].shift(1)\n",
    "        t_df = t_df.transpose()\n",
    "        header = t_df.iloc[0]\n",
    "        for number, head in enumerate(header):\n",
    "            if number > 0:\n",
    "                header[number] = re.sub(r'\\-', \"\", head)\n",
    "                \n",
    "                \n",
    "        header = pd.concat([pd.Series([\"he3\"]),header[1:]])\n",
    "        df_naming_mask = t_df[2:]\n",
    "        df_naming_mask.columns = header\n",
    "        AC_df_final = df_naming_mask.set_index(times)\n",
    "        \n",
    "        ##################################################################################################################\n",
    "        # Handle the last table from each split section\n",
    "        FP_handling = re.split(r\"\\-{6,}\",data_from_case[3]) #Gets rid of everything after the actual .out file\n",
    "        \n",
    "        #Back to what we've seen before (if you've been paying attention ;)\n",
    "        temp_data = FP_handling[0].split('\\n')\n",
    "        t_df = pd.DataFrame(temp_data)\n",
    "        t_df = t_df[0].str.split(\" * \", expand = True)\n",
    "        t_df.iloc[0] = t_df.iloc[0].shift(1)\n",
    "        t_df = t_df.transpose()\n",
    "        header = t_df.iloc[0]\n",
    "        for number, head in enumerate(header):\n",
    "            if number > 0:\n",
    "                header[number] = re.sub(r'\\-', \"\", head)\n",
    "        header = pd.concat([pd.Series([\"he3\"]),header[1:]])\n",
    "        df_naming_mask = t_df[2:]\n",
    "        df_naming_mask.columns = header\n",
    "#         if number == 0:\n",
    "#             df_naming_mask = t_df\n",
    "#             df_naming_mask.columns = header\n",
    "#             FP_df_final = df_naming_mask.set_index(times)\n",
    "#         else:\n",
    "#             df_naming_mask = t_df[2:]\n",
    "#             df_naming_mask.columns = header\n",
    "        FP_df_final = df_naming_mask.set_index(times).iloc[1:] #This is very close to doing what I want\n",
    "            \n",
    "#         FP_df_final = df_naming_mask.set_index(times).iloc[:-1]\n",
    "        \n",
    "        ##################################################################################################################\n",
    "        ###################################\n",
    "        # Append the new data to the big dataframe with each data type\n",
    "        ###################################\n",
    "        All_LE = pd.concat([All_LE,LE_df_final],sort = True)\n",
    "        All_AC = pd.concat([All_AC,AC_df_final],sort = True)\n",
    "        All_FP = pd.concat([All_FP,FP_df_final],sort = True)\n",
    "\n",
    "    print(\"All cases stored in DataFrames as strings\")\n",
    "\n",
    "    #We ignore LE for our purposes. But you can see there is some clear repitition here.\n",
    "    \n",
    "    #Drop empty column\n",
    "    All_FP = All_FP.drop([0,''], axis = 1)\n",
    "    All_AC = All_AC.drop([0,''], axis = 1)\n",
    "    \n",
    "    # DEBUGGING drop the last row to curb duplicates.\n",
    "    All_FP.drop(All_FP.tail(1).index,inplace=True) # drop last n rows\n",
    "    All_AC.drop(All_AC.tail(1).index,inplace=True) # drop last n rows\n",
    "    \n",
    "    #Convert the index from strings to floats\n",
    "    All_FP.index = pd.to_numeric(All_FP.index)#, downcast=\"float\")\n",
    "    All_AC.index = pd.to_numeric(All_AC.index)#, downcast=\"float\")\n",
    "\n",
    "    print(\"Converting strings to floats in DataFrame\")\n",
    "    \n",
    "    for column in All_AC:\n",
    "        #Converting strings to floats in DataFrame\n",
    "        All_AC[All_AC[column].name] = pd.to_numeric(All_AC[All_AC[column].name])\n",
    "        \n",
    "    print(\"Actinides complete\")\n",
    "    \n",
    "    for column in All_FP:\n",
    "        All_FP[All_FP[column].name] = pd.to_numeric(All_FP[All_FP[column].name])\n",
    "        \n",
    "    print(\"Fission Products complete\")\n",
    "\n",
    "    #Convert the index to a new column named Time ({Days/Years})\n",
    "    All_FP.insert(loc = 0, column = 'Time ({})'.format(time_units),value = All_FP.index)\n",
    "    All_AC.insert(loc = 0, column = 'Time ({})'.format(time_units),value = All_AC.index)\n",
    "\n",
    "    #Here, NOI is to identify the Type of isotope of interest\n",
    "    if NOI == 'FP':\n",
    "        return All_FP\n",
    "    if NOI == 'AC':\n",
    "        return All_AC\n",
    "\n",
    "def dataframe_merger(List_of_DataFrames, time_units):\n",
    "    \"\"\"\n",
    "    Here List_of_DataFrames, List_of_DataFrames[0] is intended to be LEU and \n",
    "        List_of_DataFrames[1] is intended to be NU\n",
    "    \"\"\"\n",
    "    print(\"Here are the time units: {}\".format(time_units))\n",
    "    if Portion_of_core == 1:\n",
    "        List_of_DataFrames[0] = List_of_DataFrames[0].drop(columns = [\"Time ({})\".format(time_units)])\n",
    "        List_of_DataFrames[0][\"Total_Activity\"] = List_of_DataFrames[0].iloc[:, 1:].sum(axis=1)\n",
    "        List_of_DataFrames[0] = List_of_DataFrames[0].sort_values(by = List_of_DataFrames[0].iloc[-1].name, axis = 1, ascending = False)\n",
    "#         print(List_of_DataFrames[1].head())\n",
    "        List_of_DataFrames[1][\"Total_Activity\"] = List_of_DataFrames[1].iloc[:, 1:].sum(axis=1)\n",
    "        List_of_DataFrames[1] = List_of_DataFrames[1].sort_values(List_of_DataFrames[1].iloc[-1].name, axis = 1, ascending = False)\n",
    "#         print(Total_Activity.head())\n",
    "        Total_Activity = List_of_DataFrames[0].add(List_of_DataFrames[1], fill_value = 0)\n",
    "        Total_Activity[\"Total_Activity\"] = Total_Activity.iloc[:, 1:].sum(axis=1)\n",
    "        Total_Activity = Total_Activity.sort_values(Total_Activity.iloc[-1].name, axis = 1, ascending = False)\n",
    "#         print(Total_Activity.head())\n",
    "        time1 = time.time()\n",
    "        print(\"Writing data to excel file...\")\n",
    "        print(\"Hang around, this may take a few minutes...\")\n",
    "        with pd.ExcelWriter('Total_Core.xlsx') as writer:  \n",
    "            Total_Activity.to_excel(writer, sheet_name='Total')\n",
    "            List_of_DataFrames[0].to_excel(writer, sheet_name='LEU')\n",
    "            List_of_DataFrames[1].to_excel(writer, sheet_name='NU')\n",
    "        print(\"Total_Core.xlsx saved\")\n",
    "#         print('Wow, that took {:.1f} seconds'.format(time.time()-time1))\n",
    "    if Portion_of_core == 2:\n",
    "        Total_Activity = convert_to_1kgNU(List_of_DataFrames[1], time_units = time_units)\n",
    "    return Total_Activity  \n",
    "            \n",
    "\n",
    "def obtain_inventory() :\n",
    "    print('LEU and NU inventories must be named with LEU and NU in the first characters of each respective file name.\\nAlso, there can be only the NU and LEU inventory excel documents in the directory.\\n')\n",
    "    print('\\n\\nTHIS SCRIPT ASSUMES THAT ANY ISOTOPE NOT FOUND IN THE ISOTOPE HALF-LIFE LIBRARY HAS A HALF-LIFE LESS THAN 120 DAYS!!!\\n\\n')\n",
    "    cwd = os.getcwd()\n",
    "    Files = os.listdir(cwd)\n",
    "    excel_list = list()\n",
    "    for file in Files :\n",
    "        if '.' in file :\n",
    "            if file.split('.')[1] == 'xlsx' :\n",
    "                excel_list.append(file)\n",
    "\n",
    "    excel_list.sort()\n",
    "    for i,excel_files in enumerate(excel_list) :\n",
    "        if excel_files == 'The_Hottest_1kgNU_Inventory.xlsx' or excel_files == 'Total_core.xlsx':\n",
    "            continue\n",
    "        if i == 0 :\n",
    "            print(f'Pulling LEU inventory from {excel_files}\\n')\n",
    "            LEU = pd.read_excel(excel_files)\n",
    "\n",
    "        elif i == 1 :\n",
    "            print(f'Pulling NU inventory from {excel_files}\\n')\n",
    "            NU = pd.read_excel(excel_files)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#   U-237 has to be entered into NU and LEU here before added to total core.\n",
    "\n",
    "    if kind_of_isotopes.lower() == 'actinides' or kind_of_isotopes.lower() == 'an' :\n",
    "        print('Computing U-237 activity as a function of time and adding it to the inventories.\\n')\n",
    "        NU,LEU = U_237_adder(NU,LEU)\n",
    "\n",
    "    if Portion_of_core == 1 :\n",
    "        print(f'Adding the LEU and NU dataframes.\\n')\n",
    "        Total_core = NU.add(LEU,fill_value = 0)\n",
    "        Time = pd.Series.to_numpy(NU['Unnamed: 0'])\n",
    "        print(Time)\n",
    "        Total_core = Total_core.drop(columns = ['Unnamed: 0'])\n",
    "        # Sorting the dataframe by highest endpoint activity\n",
    "        Total_core = pd.DataFrame.sort_values(Total_core,Total_core.shape[0]-1,axis = 1,ascending = False)\n",
    "        Time_column_name = f'Time ({time_units})'\n",
    "        Total_core.insert(loc=0,column = Time_column_name,value= Time)\n",
    "        # Writing the total core excel spreadsheet\n",
    "        print(f'Writing total inventory to Total_core.xlsx\\n')\n",
    "        Total_core.reset_index(drop=True, inplace=True)\n",
    "        Total_core.to_excel('Total_core.xlsx')\n",
    "        print(f'Total Core inventory saved in Total_core.xlsx\\n')\n",
    "        return Total_core\n",
    "    if Portion_of_core == 2 :\n",
    "        print(f'Converting NU inventory into hottest 1 kgNU.\\n')\n",
    "\n",
    "        Inventory = convert_to_1kgNU(NU)\n",
    "\n",
    "        return Inventory\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def half_life_greater_than(isotope) :\n",
    "    with open('Half_Lives_List.txt','r') as file_handle :\n",
    "        bool = False\n",
    "        for line in file_handle :\n",
    "            if line.split()[0] == isotope:\n",
    "                half_life = line.split()[3]\n",
    "\n",
    "                # print(isotope, float(half_life) >= 86400*120.0)\n",
    "                if float(half_life) >= 86400*120.0 :\n",
    "                    bool = True\n",
    "                    return True\n",
    "                else :\n",
    "                    bool = True\n",
    "                    return False\n",
    "\n",
    "        if bool == False :\n",
    "            return False\n",
    "            raise ValueError(f'Isotope {isotope} not found in half_lives_list.txt')\n",
    "\n",
    "\n",
    "def split_120(original) :\n",
    "\n",
    "\n",
    "    Greater_than = pd.DataFrame()\n",
    "    Less_than = pd.DataFrame()\n",
    "\n",
    "    isotopes = list(original)\n",
    "    time_identifier = f'Time ({time_units})'\n",
    "\n",
    "    for isotope in isotopes :\n",
    "\n",
    "\n",
    "        if isotope == 'Totals' or isotope == 'Subtotals':\n",
    "\n",
    "            continue\n",
    "\n",
    "        elif isotope != time_identifier :\n",
    "            if half_life_greater_than(isotope) :\n",
    "                Greater_than.insert(0,isotope,original[isotope])\n",
    "            else :\n",
    "                Less_than.insert(0,isotope,original[isotope])\n",
    "\n",
    "        else :\n",
    "            pass\n",
    "            #raise ValueError(f'Isotope {isotope} passed if tests.\\n')\n",
    "\n",
    "\n",
    "    total_list = list()\n",
    "\n",
    "    for key,value in Greater_than.iterrows() :\n",
    "        sum = 0.0\n",
    "        for vals in value :\n",
    "            sum += vals\n",
    "        total_list.append(sum)\n",
    "    Greater_than.insert(0,'Total',total_list)\n",
    "\n",
    "    total_list = list()\n",
    "\n",
    "    for key,value in Less_than.iterrows() :\n",
    "        sum = 0.0\n",
    "        for vals in value :\n",
    "            sum += vals\n",
    "        total_list.append(sum)\n",
    "    Less_than.insert(0,'Total',total_list)\n",
    "\n",
    "    return Greater_than, Less_than\n",
    "print('The Inventory Plotter: Chad Denbrock, Niowave Inc. August 2020\\n\\n')\n",
    "while True :\n",
    "    time_units = input('What are the time units given in the excel files? (e.g. Days or Years)\\n')\n",
    "    time_units = time_units.lower()\n",
    "#    if time_units.lower() == 'years' :\n",
    "    if time_units == 'years':\n",
    "        break\n",
    "#    elif time_units.lower() == 'days' :\n",
    "    elif time_units == 'days':\n",
    "        break\n",
    "    else :\n",
    "        print('The time units must be either days or years. Try again.\\n')\n",
    "\n",
    "#       Plotting fission products or actinides?\n",
    "while True :\n",
    "    kind_of_isotopes = input('What are you plotting? (Fission Products = FP or Actinides = An)\\n')\n",
    "    if kind_of_isotopes.lower() == 'fission products' or kind_of_isotopes.lower() == 'fp' :\n",
    "        Title = 'Fission Products'\n",
    "        fname = 'Fission-Products_UTA-2'\n",
    "        plotting_type = \"FP\"\n",
    "        break\n",
    "    elif kind_of_isotopes.lower() == 'actinides' or kind_of_isotopes.lower() == 'an':\n",
    "        Title = 'Actinides'\n",
    "        fname = 'Actinides_UTA-2'\n",
    "        plotting_type = \"AC\"\n",
    "        break\n",
    "    else :\n",
    "        print('You must be plotting either Actinides or fission products, not both or neither.')\n",
    "while True :\n",
    "    Portion_of_core = int(input('What portion of the core would you like to analyze (1 or 2)? \\n'\\\n",
    "                                '1: Whole Core (LEU + NU)\\n'\\\n",
    "                                '2: Dispersable (Hottest 1 kgNU)\\n'))\n",
    "    if Portion_of_core == 1:\n",
    "        Title = Title + ' in the Whole Core'\n",
    "        fname = fname + '_whole_core'\n",
    "        break\n",
    "    elif Portion_of_core == 2:\n",
    "        Title = Title + ' in the hottest 1 kgNU'\n",
    "        fname = fname + '_hottest_1kgNU'\n",
    "        break\n",
    "    else :\n",
    "        print('Type 1 or 2 as your selection for what part of the core you would like to analyze.\\n')\n",
    "files = glob.glob(\"*.out\")\n",
    "print(\"#\"*60)\n",
    "print(\"---LEU---\")\n",
    "print(\"#\"*60)\n",
    "while True:\n",
    "    try:\n",
    "        print(\"Please select which output file to analyze (LEU)\")\n",
    "        for number,file in enumerate(files):\n",
    "            print(\"{}: {}\\n\".format(number, file))\n",
    "        file_number_LEU = int(input(\"\"))\n",
    "        print(\"Loading {}\".format(files[file_number_LEU]))\n",
    "        break\n",
    "    except:\n",
    "        print(\"Please enter an integer corresponding to the filename above\")\n",
    "        continue\n",
    "\n",
    "# The_Inventory = obtain_inventory()\n",
    "\n",
    "\n",
    "print(\"#\"*60)\n",
    "print(\"---NU---\")\n",
    "print(\"#\"*60)\n",
    "while True:\n",
    "    try:\n",
    "        print(\"Please select which output file to analyze (NU)\")\n",
    "        for number,file in enumerate(files):\n",
    "            print(\"{}: {}\\n\".format(number, file))\n",
    "        file_number_NU = int(input(\"\"))\n",
    "        print(\"Loading {}\".format(files[file_number_NU]))\n",
    "        break\n",
    "    except:\n",
    "        print(\"Please enter an integer corresponding to the filename above\")\n",
    "        continue\n",
    "        \n",
    "#Run it\n",
    "###############################################################################\n",
    "LEU = output_reader(files[file_number_LEU], plotting_type, time_units = time_units.lower())\n",
    "NU = output_reader(files[file_number_NU], plotting_type, time_units = time_units.lower())\n",
    "dfs = dataframe_merger([LEU, NU], time_units = time_units.lower())\n",
    "The_Inventory = dfs\n",
    "plotting(The_Inventory)\n",
    "print(\"Inventory Plotter Complete\")\n",
    "# time.sleep(600)\n",
    "print(Time-time.time())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
